{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Простые счётчики на службе векторной семантики\n",
    "## Скачиваем текст"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import urllib\n",
    "from urllib import request\n",
    "\n",
    "text_data = request.urlopen(\"http://www.gutenberg.org/files/1399/1399-0.txt\").read().decode(\"UTF-8\") + \" \"\n",
    "text_data += request.urlopen(\"https://archive.org/stream/warandpeace030164mbp/warandpeace030164mbp_djvu.txt\").read().decode(\"UTF-8\")\n",
    "\n",
    "text_data[:100]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Убираем теги"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "clean_data = re.sub(\"<[^>]*>\", \" \", text_data)\n",
    "clean_data = re.sub(\"\\s+\", \" \", text_data)\n",
    "clean_data[:100]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Бьём текст на предложения"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import nltk.data\n",
    "\n",
    "# загружаем токенизатор\n",
    "sent_detector = nltk.data.load('tokenizers/punkt/english.pickle')\n",
    "sentences = sent_detector.tokenize(clean_data.strip())\n",
    "\n",
    "print(\"Total number of sentences:\", len(sentences))\n",
    "\n",
    "sentences[:2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Разбиваем предложения на токены"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import TweetTokenizer\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "import re\n",
    "\n",
    "nltk.download('wordnet')\n",
    "\n",
    "# можно попробовать и какой-нибудь другой\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "# можно попробовать и какой-нибудь другой\n",
    "tokenizer = TweetTokenizer()\n",
    "splitted = []\n",
    "\n",
    "for sent in sentences:\n",
    "    splitted.append([lemmatizer.lemmatize(w).lower() for w in tokenizer.tokenize(sent) if re.match(\"^[A-Za-z'-]+$\", w)])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Своеобразный способ построить словарь"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "vectorizer = CountVectorizer(tokenizer=lambda x: x.split(' '), min_df=25)\n",
    "term_doc_matrix = vectorizer.fit_transform([\" \".join(sentence) for sentence in splitted])\n",
    "\n",
    "term2id = vectorizer.vocabulary_\n",
    "id2term = {v: k for k, v in term2id.items()}\n",
    "\n",
    "print(\"Term-document matrix shape:\", term_doc_matrix.shape)\n",
    "print(\"Vocabulary samples:\", list(term2id.items())[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Посчитаем частоты термов, используя построенную матрицу"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "term_counts = term_doc_matrix.sum(axis=0).A1\n",
    "print(term_counts.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---- \n",
    "\n",
    "### Опциональное задание: выбросьте слишком частотные и слишком редкие термы\n",
    "\n",
    "----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#todo: hint: надо использовать term_counts, term2id + перестроить все матрицы"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Строим term-context matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import scipy.sparse as sp\n",
    "\n",
    "half_window = 5\n",
    "\n",
    "X = np.zeros(shape=(len(term2id), len(term2id)))\n",
    "\n",
    "\n",
    "for sentence in splitted:\n",
    "    \n",
    "    # бежим с индексом по предложению\n",
    "    for i in range(len(sentence)):        \n",
    "        current_word = sentence[i]      \n",
    "        \n",
    "        # если слова нет в словаре, ничего для него не считаем\n",
    "        if current_word in term2id:\n",
    "            word_idx = term2id[current_word]\n",
    "\n",
    "            for c in range(-half_window, half_window):\n",
    "                current_idx = i + c\n",
    "\n",
    "                # проверяем, не наткнулись ли на границы предложения\n",
    "                if 0 <= current_idx < len(sentence):\n",
    "                    context_word = sentence[current_idx]\n",
    "                    \n",
    "                    if context_word in term2id:                    \n",
    "                        context_idx = term2id[context_word]\n",
    "                        X[word_idx, context_idx] += 1\n",
    "\n",
    "print(\"Sparsity of the term-context matrix\", len(X.nonzero()[0]) / (X.shape[0] ** 2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Функция поиска ближайших K"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.spatial.distance import cdist\n",
    "\n",
    "def dict_k_closest(M, term_dict, inverse_term_dict, k=5):\n",
    "    \"\"\"\n",
    "        :param M -- матрица векторых представлений\n",
    "        :param term_dict -- слово2индекс\n",
    "        :param inverse_term_dict -- индекс2слово\n",
    "        :param k -- число ближайших соседей для выдачи\n",
    "    \"\"\"\n",
    "    \n",
    "    print(\"Computing all distances... (takes some time)\")    \n",
    "    distances = cdist(M, M, \"cosine\")\n",
    "    sorted_by_dist_k = np.argsort(distances, axis=1)[:, :k]\n",
    "    \n",
    "    results = {}\n",
    "    \n",
    "    for term in term_dict:\n",
    "        row_id = term_dict[term]\n",
    "        similar = [inverse_term_dict[i] for i in sorted_by_dist_k[row_id, :]]\n",
    "        results[term] = similar   \n",
    "        \n",
    "    return results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Смотрим глазами на наши успехи"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "similar_terms = dict_k_closest(X, term2id, id2term)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for key in list(similar_terms)[:10]:\n",
    "    print(key, \":\", \" \".join(similar_terms[key][1:]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ну как вам?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(Правильный ответ: не очень)\n",
    "\n",
    "\n",
    "### PMI: Pointwise Mutual Information\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_PMI = X.copy()\n",
    "total_bicount = X_PMI.sum()\n",
    "total_unicount = term_counts.sum()\n",
    "\n",
    "# p(x,y) / p(x) / p(y)\n",
    "X_PMI = (X_PMI / total_bicount) / (term_counts[:, None] + 0.000001) / (term_counts[None, :] + 0.000001)  * total_unicount ** 2\n",
    "X_PMI = np.where(X_PMI > 0, np.log2(X_PMI), 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "similar_terms = dict_k_closest(X_PMI, term2id, id2term)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for key in list(similar_terms)[:10]:\n",
    "    print(key, \":\", \" \".join(similar_terms[key][1:]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Positive PMI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_PPMI = X_PMI.copy()\n",
    "X_PPMI[X_PPMI < 0] = 0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "similar_terms = dict_k_closest(X_PPMI, term2id, id2term)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for key in list(similar_terms)[:10]:\n",
    "    print(key, \":\", \" \".join(similar_terms[key][1:]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ну как?\n",
    "Должно стать лучше"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Применим Truncated SVD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# U, S, Vh = np.linalg.svd(X_PPMI, full_matrices=False)\n",
    "# U.shape, S.shape, Vh.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from  sklearn.decomposition import TruncatedSVD\n",
    "\n",
    "factorizer = TruncatedSVD(n_components=200, random_state=0)\n",
    "\n",
    "# матрица слов\n",
    "W = factorizer.fit_transform(X_PPMI)\n",
    "print(W.shape)\n",
    "\n",
    "# матрица контекстов\n",
    "C = factorizer.components_\n",
    "print(C.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "similar_terms = dict_k_closest(W, term2id, id2term)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for key in list(similar_terms)[:10]:\n",
    "    print(key, \":\", \" \".join(similar_terms[key][1:]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Попробуем разложить в произведение двух матриц"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from  sklearn.decomposition import NMF\n",
    "\n",
    "factorizer = NMF(n_components=200, random_state=0)\n",
    "\n",
    "# матрица слов\n",
    "W = factorizer.fit_transform(X_PPMI)\n",
    "print(W.shape)\n",
    "\n",
    "# матрица контекстов\n",
    "C = factorizer.components_\n",
    "print(C.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "similar_terms = dict_k_closest(W, term2id, id2term)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for key in list(similar_terms)[:10]:\n",
    "    print(key, \":\", \" \".join(similar_terms[key][1:]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Задача техническая: сохранить векторы в CSV\n",
    "---- "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Посмотрим, как можно оценивать качество\n",
    "Отличный источник, горячо рекомендуется\n",
    "https://github.com/EloiZ/embedding_evaluation\n",
    "\n",
    "Надо склонировать репозиторий, загрузить датасеты с помощью\n",
    "`download_benchmarks.py`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.environ[\"EMBEDDING_EVALUATION_DATA_PATH\"] = \"embedding_evaluation/data/\"\n",
    "\n",
    "import embedding_evaluation\n",
    "from embedding_evaluation.evaluate import Evaluation\n",
    "from embedding_evaluation.load_embedding import load_embedding_textfile\n",
    "\n",
    "def eval_word_vectors(path):\n",
    "    # Load embeddings as a dictionnary {word: embed} where embed is a 1-d numpy array.\n",
    "    embeddings = load_embedding_textfile(textfile_path=path)\n",
    "\n",
    "    # Load and process evaluation benchmarks\n",
    "    evaluation = Evaluation() \n",
    "\n",
    "    return evaluation.evaluate(embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Задача: оценить качество сохранённых моделей, попытаться его улучшить"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mymodelresults = eval_word_vectors(\"trali-vali.csv\")\n",
    "\n",
    "mymodelresults"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  },
  "toc": {
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": "block",
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
