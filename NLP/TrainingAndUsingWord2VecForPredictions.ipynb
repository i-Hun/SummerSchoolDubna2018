{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Применяем word2vec на практике"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-07-27T08:16:40.395196Z",
     "start_time": "2018-07-27T08:16:40.384139Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "%load_ext autoreload\n",
    "%autoreload 3\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**gensim** не зря считают библиотекой \"с человеческим лицом\" для topic modeling и vector semantics.\n",
    "\n",
    "Простой интерфейс и ПОРАЗИТЕЛЬНО высокая скорость обучения *word2vec*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-07-27T08:16:42.427934Z",
     "start_time": "2018-07-27T08:16:41.700028Z"
    }
   },
   "outputs": [],
   "source": [
    "import gensim\n",
    "import numpy as np  \n",
    "from gensim.models import Word2Vec\n",
    "\n",
    "def train_word2vec(prefix, sentences, num_features=300, \n",
    "                   min_word_count=5, num_workers=4, \n",
    "                   context=10, downsampling=1e-3, save=True, sg=1):\n",
    "\n",
    "    # обучение\n",
    "    print(\"Training Word2Vec model...\")\n",
    "    \n",
    "    model = Word2Vec(sentences, workers=num_workers, \\\n",
    "                size=num_features, min_count=min_word_count, \\\n",
    "                window=context, sample=downsampling, seed=1, sg=sg)\n",
    "\n",
    "    # сделаем модель поменьше в RAM\n",
    "    model.init_sims(replace=True)\n",
    "\n",
    "    if save:\n",
    "        model_name = prefix + \"_\" + str(num_features) + \"features_\" + str(min_word_count) + \"minwords_\" + str(context) + \"context\"\n",
    "        model.save(model_name)\n",
    "        print(\"Model\", model_name, \"saved\")\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n",
    "## Обучаем word2vec на наших текстах\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading\n",
      "Parsing\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Done'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import urllib\n",
    "import urllib.request\n",
    "import re\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "print(\"Downloading\")\n",
    "\n",
    "# Толстой \n",
    "with open(\"/Users/hun/Google Drive/Pubs_and_confs/Летняя школа 2018/materials/NLP/file1.txt\") as f:\n",
    "    wp_txt = f.read()\n",
    "#wp_txt = urllib.request.urlopen(\"https://www.gutenberg.org/files/2600/2600-h/2600-h.htm\")\n",
    "\n",
    "print(\"Parsing\")\n",
    "soup = BeautifulSoup(wp_txt)\n",
    "\n",
    "# print(\"Cleaning\")\n",
    "# wp_txt = soup.find('body').get_text()\n",
    "# \n",
    "# print(\"Downloading\")\n",
    "# ak_txt = urllib.request.urlopen(\"http://www.gutenberg.org/files/1399/1399-0.txt\")\n",
    "# ak_txt = ak_txt.read().decode(\"utf-8\")\n",
    "\n",
    "\n",
    "txt = wp_txt\n",
    "\"Done\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Альтернативные данные"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-07-27T08:16:52.333114Z",
     "start_time": "2018-07-27T08:16:52.292817Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nThe Project Gutenberg EBook of Anna Karenina, by Leo Tolstoy\\n\\nThis eBook is for the use of anyone a'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    "txt = open(\"file1.txt\").read() + \" \" + open(\"file2.txt\").read()  + \" \" + open(\"file3.txt\").read()\n",
    "\n",
    "txt[:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-07-27T08:16:55.536842Z",
     "start_time": "2018-07-27T08:16:55.526527Z"
    }
   },
   "outputs": [],
   "source": [
    "def prepare_sentences(txt, word_threshold=2, stage_train=True):\n",
    "\n",
    "    # вычищаем переносы\n",
    "    whitespaces = re.compile(\"\\s+\", re.U)\n",
    "    txt = re.sub(\"\\s+\", \" \", txt).lower()\n",
    "\n",
    "    # убираем всё, кроме \"слов\", разбив на предложения\n",
    "    sentences = re.split(\"[!\\?\\.]+\", txt.replace(\"\\n\", \" \"))\n",
    "    clean_sentences = [re.split(\"\\W+\", s) for s in sentences]\n",
    "    clean_sentences = [[w.replace(\"\\d+\", \"NUM\") for w in s if w] for s in clean_sentences]\n",
    "    \n",
    "    if stage_train:\n",
    "\n",
    "        counter = Counter()\n",
    "\n",
    "        for s in clean_sentences:\n",
    "            for w in s:\n",
    "                counter[w] += 1\n",
    "    \n",
    "        print(\"Filtered out word types :\", len([w for w in counter if counter[w] <= word_threshold]))\n",
    "        print(\"Filtered out words count:\", sum([counter[w] for w in counter if counter[w] <= word_threshold]))\n",
    "    \n",
    "        # выкидываем редкие, и заменяем их на специальный тег\n",
    "        clean_sentences = [[w if counter[w] > word_threshold else UNK for w in s] for s in clean_sentences]            \n",
    "    \n",
    "    word2index = { }\n",
    "    index2word = { }\n",
    "    \n",
    "    counter = max(word2index.values() if word2index else [0]) + 1\n",
    "\n",
    "    for s in clean_sentences:\n",
    "        for w in s:\n",
    "            if not w in word2index:\n",
    "                word2index[w] = counter\n",
    "                index2word[counter] = w\n",
    "                counter += 1\n",
    "                \n",
    "    return word2index, index2word, clean_sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-07-27T08:16:57.705880Z",
     "start_time": "2018-07-27T08:16:56.417290Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(42638,\n",
       " 'the project gutenberg ebook of anna karenina by leo tolstoy this ebook is for the use of anyone anywhere at no cost and with almost no restrictions whatsoever')"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word2index, index2word, clean_sentences = prepare_sentences(txt=txt, stage_train=False)\n",
    "\n",
    "len(clean_sentences), \" \".join(clean_sentences[:1][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-07-27T08:17:02.472326Z",
     "start_time": "2018-07-27T08:16:58.423307Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Word2Vec model...\n",
      "Model tolstoy__45features_10minwords_7context saved\n"
     ]
    }
   ],
   "source": [
    "w2v_model = train_word2vec(sentences=clean_sentences, \n",
    "                           prefix=\"tolstoy_\", \n",
    "                           context=7, downsampling=0.0001,\n",
    "                           min_word_count=10,\n",
    "                           num_features=45)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-07-27T08:19:56.956450Z",
     "start_time": "2018-07-27T08:19:56.949798Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('rather', 0.8806427717208862),\n",
       " ('woman', 0.878503680229187),\n",
       " ('confident', 0.868535578250885),\n",
       " ('satisfaction', 0.8645564317703247),\n",
       " ('a', 0.8581892848014832),\n",
       " ('powerful', 0.8566306233406067),\n",
       " ('majestic', 0.8547239303588867),\n",
       " ('un', 0.8546481132507324),\n",
       " ('young', 0.8535899519920349),\n",
       " ('with', 0.8520524501800537)]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w2v_model.most_similar(\"anna\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Посмотрим, как можно оценивать качество\n",
    "Отличный источник, горячо рекомендуется\n",
    "https://github.com/EloiZ/embedding_evaluation\n",
    "\n",
    "Надо склонировать репозиторий, загрузить датасеты с помощью\n",
    "`download_benchmarks.py`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-07-27T08:23:37.841373Z",
     "start_time": "2018-07-27T08:23:37.832142Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('anna', 1.0000001192092896),\n",
       " ('pdvlovna', 0.9463701248168945),\n",
       " ('countess', 0.9375899434089661),\n",
       " ('prin', 0.9286930561065674),\n",
       " ('mikhdylovna', 0.9284204244613647),\n",
       " ('nicholas', 0.9226316809654236),\n",
       " ('natdsha', 0.9201843738555908),\n",
       " ('anatole', 0.9178056120872498),\n",
       " ('levin', 0.9176115393638611),\n",
       " ('boris', 0.9090338349342346)]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w2v_model.similar_by_vector(w2v_model.wv[\"anna\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-07-27T08:34:12.172137Z",
     "start_time": "2018-07-27T08:34:12.164917Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('man', 0.8913528919219971),\n",
       " ('a', 0.8841538429260254),\n",
       " ('german', 0.8617697954177856),\n",
       " ('schröder', 0.8510404825210571),\n",
       " ('il', 0.8465536236763),\n",
       " ('captain', 0.8427188992500305),\n",
       " ('master', 0.8426240682601929),\n",
       " ('martini', 0.8416328430175781),\n",
       " ('surgeon', 0.8406222462654114),\n",
       " ('fine', 0.8396058678627014)]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w2v_model.similar_by_vector(w2v_model.wv[\"he\"] - w2v_model.wv[\"she\"] + w2v_model.wv[\"woman\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-07-27T08:19:06.865061Z",
     "start_time": "2018-07-27T08:19:06.607577Z"
    }
   },
   "outputs": [],
   "source": [
    "import csv\n",
    "\n",
    "# сохраним в CSV\n",
    "with open(\"tolstoy__100features_3minwords_5context.csv\", \"w+\") as wf:\n",
    "    writer = csv.writer(wf)\n",
    "    for word in w2v_model.wv.vocab:\n",
    "        writer.writerow([word] + [v for v in  w2v_model.wv[word]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-07-27T08:19:07.444443Z",
     "start_time": "2018-07-27T08:19:07.439753Z"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.environ[\"EMBEDDING_EVALUATION_DATA_PATH\"] = \"embedding_evaluation/data/\"\n",
    "\n",
    "import embedding_evaluation\n",
    "from embedding_evaluation.evaluate import Evaluation\n",
    "from embedding_evaluation.load_embedding import load_embedding_textfile\n",
    "\n",
    "def eval_word_vectors(path):\n",
    "    # Load embeddings as a dictionnary {word: embed} where embed is a 1-d numpy array.\n",
    "    embeddings = load_embedding_textfile(textfile_path=path)\n",
    "\n",
    "    # Load and process evaluation benchmarks\n",
    "    evaluation = Evaluation() \n",
    "\n",
    "    return evaluation.evaluate(embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-07-27T08:19:10.946864Z",
     "start_time": "2018-07-27T08:19:09.569548Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'similarity': {'usf': {'all_entities': 0.12484655136907434,\n",
       "   'entity_subset': 0},\n",
       "  'ws353': {'all_entities': 0.183037916288782, 'entity_subset': 0},\n",
       "  'men': {'all_entities': 0.4282341451049674, 'entity_subset': 0},\n",
       "  'vis_sim': {'all_entities': 0.16981296514614985, 'entity_subset': 0},\n",
       "  'sem_sim': {'all_entities': 0.1876917877835622, 'entity_subset': 0},\n",
       "  'simlex': {'all_entities': 0.17260042899681838, 'entity_subset': 0}},\n",
       " 'concreteness': 0.323137472651751}"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tolstoy = eval_word_vectors(\"tolstoy__100features_3minwords_5context.csv\")\n",
    "\n",
    "tolstoy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Задание\n",
    "\n",
    "Здесь везде -- more is better. Попробуйте настроить модель так, чтобы similarity выросла.\n",
    "\n",
    "Помогают ли советы?\n",
    "\n",
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Сравним с гугловскими векторами\n",
    "\n",
    "https://code.google.com/archive/p/word2vec/\n",
    "\n",
    "\n",
    "`wget -c \"https://s3.amazonaws.com/dl4j-distribution/GoogleNews-vectors-negative300.bin.gz\"`\n",
    "\n",
    "и распаковать (но лучше взять готовую модель у меня)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-07-27T08:40:41.974542Z",
     "start_time": "2018-07-27T08:40:41.952285Z"
    }
   },
   "outputs": [],
   "source": [
    "import gensim\n",
    "from gensim.models import KeyedVectors\n",
    "\n",
    "# загружаться может долго\n",
    "# w2v_ggl = KeyedVectors.load_word2vec_format(\"GoogleNews-vectors-negative300.bin\", binary=True)\n",
    "# filtered_w2v_ggl = KeyedVectors.load_word2vec_format(\"GoogleNews-vectors-negative300.bin\", binary=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-07-27T08:55:01.676152Z",
     "start_time": "2018-07-27T08:55:01.389269Z"
    }
   },
   "outputs": [],
   "source": [
    "filtered_w2v_ggl = KeyedVectors.load(\"for_sharing/filtered-GoogleNews-vectors-negative300.bin\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-07-27T09:10:45.418756Z",
     "start_time": "2018-07-27T09:10:45.397869Z"
    }
   },
   "outputs": [],
   "source": [
    "# import gensim\n",
    "# from  gensim.models import KeyedVectors\n",
    "# filtered_w2v_ggl = KeyedVectors.load_word2vec_format(\"GoogleNews-vectors-negative300.txt\", binary=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-07-27T09:10:50.103972Z",
     "start_time": "2018-07-27T09:10:50.054578Z"
    }
   },
   "outputs": [],
   "source": [
    "from  gensim.models import KeyedVectors\n",
    "\n",
    "filtered_w2v_ggl = KeyedVectors.load(\"filtered-GoogleNews-vectors-negative300.bin\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-07-27T09:11:28.817316Z",
     "start_time": "2018-07-27T09:11:20.891808Z"
    }
   },
   "outputs": [],
   "source": [
    "gnews = eval_word_vectors(\"for_sharing/GoogleNews-vectors-negative300.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Значения метрик в читаемом виде"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-07-27T09:11:48.051736Z",
     "start_time": "2018-07-27T09:11:48.042657Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "similarity\n",
      "     usf\n",
      "       all_entities 0.3603241806749815 0.12484655136907434\n",
      "       entity_subset 0 0\n",
      "     ws353\n",
      "       all_entities 0.6873764284967581 0.183037916288782\n",
      "       entity_subset 0 0\n",
      "     men\n",
      "       all_entities 0.7447273593551881 0.4282341451049674\n",
      "       entity_subset 0 0\n",
      "     vis_sim\n",
      "       all_entities 0.5934008833294928 0.16981296514614985\n",
      "       entity_subset 0 0\n",
      "     sem_sim\n",
      "       all_entities 0.675366985636233 0.1876917877835622\n",
      "       entity_subset 0 0\n",
      "     simlex\n",
      "       all_entities 0.3471553307624142 0.17260042899681838\n",
      "       entity_subset 0 0\n",
      "concreteness 0.5782825385538641 0.323137472651751\n"
     ]
    }
   ],
   "source": [
    "for key in gnews.keys():\n",
    "    \n",
    "    if type(gnews[key]) != dict:\n",
    "        print(key, gnews[key], tolstoy[key])\n",
    "    else:\n",
    "        print(key)\n",
    "        for kk in gnews[key].keys():\n",
    "            if type(gnews[key][kk]) != dict:\n",
    "                print(\"    \", kk, gnews[key][kk], tolstoy[key][kk])\n",
    "            else:\n",
    "                print(\"    \", kk)\n",
    "                for kkk in gnews[key][kk].keys():\n",
    "                    if type(gnews[key][kk][kkk]) != dict:\n",
    "                        print(\"      \", kkk, gnews[key][kk][kkk], tolstoy[key][kk][kkk])\n",
    "                    else:\n",
    "                        print(\"      \", kkk, gnews[key][kk][kkk], tolstoy[key][kk][kkk])\n",
    "                "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word2Vec по-русски"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Нормализуем тексты"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-07-27T10:12:02.335172Z",
     "start_time": "2018-07-27T10:12:02.258877Z"
    }
   },
   "outputs": [],
   "source": [
    "import re\n",
    "import pymorphy2 \n",
    "from pymorphy2.tokenizers import *\n",
    "from functools import lru_cache\n",
    "from tqdm import tqdm\n",
    "\n",
    "LEMMATIZER = pymorphy2.MorphAnalyzer()\n",
    "\n",
    "tags = re.compile(\"<[^>]*>\")\n",
    "html_codes = re.compile(\"&\\w+;\")\n",
    "nums = re.compile(\"\\d+\")\n",
    "nonalpha = re.compile(\"[A-Za-z0-9ёЁ]+\", re.U)\n",
    "\n",
    "def remove_html(txt):\n",
    "    return html_codes.sub(\" \", tags.sub(\" \", txt))\n",
    "\n",
    "def replace_nums(txt):\n",
    "    return nums.sub(\"<num>\", txt)\n",
    "\n",
    "def remove_nonalpha(txt):\n",
    "    return nonalpha.sub(\" \", txt)\n",
    "\n",
    "def tokenize(text):\n",
    "    tokens = [t for t in simple_word_tokenize(text) if not nonalpha.match(t)]\n",
    "    return tokens\n",
    "\n",
    "\n",
    "@lru_cache(maxsize=1000000)\n",
    "def lemmatize(word):\n",
    "    p = LEMMATIZER.parse(word)[0]\n",
    "    return p.normal_form, p.tag\n",
    "\n",
    "\n",
    "def lemmatize_text(split_text):\n",
    "    return re.sub(\"\\s+\", \" \", \" \".join([lemmatize(t)[0] for t in split_text]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-07-27T10:12:04.958832Z",
     "start_time": "2018-07-27T10:12:03.458076Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'analysis': [{'lex': 'корабль', 'wt': 1, 'gr': 'S,муж,неод=(вин,ед|им,ед)'}],\n",
       "  'text': 'Корабль'},\n",
       " {'text': ' '},\n",
       " {'analysis': [{'lex': 'давать', 'wt': 1, 'gr': 'V,пе=прош,ед,изъяв,муж,сов'}],\n",
       "  'text': 'дал'},\n",
       " {'text': ' '},\n",
       " {'analysis': [{'lex': 'течь',\n",
       "    'wt': 0.2378379256,\n",
       "    'gr': 'S,жен,неод=(вин,ед|им,ед)'}],\n",
       "  'text': 'течь'},\n",
       " {'text': '\\n'}]"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pymystem3\n",
    "ma = pymystem3.Mystem()\n",
    "\n",
    "ma.analyze(\"Корабль дал течь\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-07-27T10:12:04.972461Z",
     "start_time": "2018-07-27T10:12:04.963136Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'analysis': [{'lex': 'волга', 'wt': 1, 'gr': 'S,жен,неод=им,ед'}],\n",
       "  'text': 'Волга'},\n",
       " {'text': ' '},\n",
       " {'analysis': [{'lex': 'всегда', 'wt': 1, 'gr': 'ADVPRO='}], 'text': 'всегда'},\n",
       " {'text': ' '},\n",
       " {'analysis': [{'lex': 'быть',\n",
       "    'wt': 0.149256438,\n",
       "    'gr': 'V,нп=непрош,ед,изъяв,3-л'}],\n",
       "  'text': 'будет'},\n",
       " {'text': ' '},\n",
       " {'analysis': [{'lex': 'течь', 'wt': 0.7621620893, 'gr': 'V,несов,нп=инф'}],\n",
       "  'text': 'течь'},\n",
       " {'text': '\\n'}]"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ma.analyze(\"Волга всегда будет течь\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-07-27T10:12:07.667922Z",
     "start_time": "2018-07-27T10:12:04.976064Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading\n",
      "Parsing\n"
     ]
    }
   ],
   "source": [
    "import zipfile\n",
    "import urllib.request\n",
    "\n",
    "print(\"Downloading\")\n",
    "\n",
    "# сжатый Толстой \n",
    "url = \"https://aldebaran.ru/author/tolstoyi_lev/kniga_anna_karenina1878_ru/download.html.zip\"\n",
    "ak_ru_zip = urllib.request.urlopen(url).read()\n",
    "\n",
    "with open(\"karenina.zip\", \"wb\") as wf:\n",
    "    wf.write(ak_ru_zip)\n",
    "\n",
    "with zipfile.ZipFile(\"karenina.zip\",\"r\") as zip_ref:\n",
    "    zip_ref.extractall(\"karenina_ru\")\n",
    "\n",
    "print(\"Parsing\")\n",
    "\n",
    "html = open(\"karenina_ru/Tolstoyi_L._Anna_KareninaI.html\", encoding=\"windows-1251\").read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-07-27T09:42:59.937986Z",
     "start_time": "2018-07-27T09:42:59.932235Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'м прямо глядя в глаза Облонскому.<br></br>\\xa0\\xa0\\xa0–\\xa0Ну, хорошо, хорошо. Погоди еще, и ты придешь к этому. Хорошо, как у тебя три тысячи десятин в Каразинском уезде, да такие мускулы, да свежесть, как у двенадцатилетней девочки,\\xa0– а придешь и ты к нам. Да, так о том, что ты спрашивал: перемены нет, но жаль, что ты так давно не был.<br></br>\\xa0\\xa0\\xa0–\\xa0А что?\\xa0– испуганно спросил Левин.<br></br>\\xa0\\xa0\\xa0–\\xa0Да ничего,\\xa0– отвечал Облонский.\\xa0– Мы поговорим. Да ты зачем, собственно, приехал?<br></br>\\xa0\\xa0\\xa0–\\xa0Ах, об этом тоже поговорим после,\\xa0– опять до ушей покраснев, сказал Левин.<br></br>\\xa0\\xa0\\xa0–\\xa0Ну хорошо. Понятно,\\xa0– сказал Степан Аркадьич.\\xa0– Ты видишь ли: я\\xa0бы позвал к себе, но жена не совсем здорова. А вот что: если ты хочешь их видеть, они, наверное, нынче в Зоологическом саду от четырех до пяти. Кити на коньках катается. Ты поезжай туда, а я заеду, и вместе куда-нибудь обедать.<br></br>\\xa0\\xa0\\xa0–\\xa0Прекрасно. Ну, до свидания.<br></br>\\xa0\\xa0\\xa0–\\xa0Смотри же, ты ведь, я тебя знаю, забудешь или вдруг уедешь в деревню!\\xa0– смеясь, прокричал Степан Аркадьич.<br></br>\\xa0\\xa0\\xa0–\\xa0Нет, верно.<br></br>\\xa0\\xa0\\xa0И, вспомнив о том, что он забыл поклониться товарищам Облонского, только когда он был уже в дверях, Левин вышел из кабинета.<br></br>\\xa0\\xa0\\xa0–\\xa0Должно быть, очень энергический господин,\\xa0– сказал Гриневич, когда Левин вышел.<br></br>\\xa0\\xa0\\xa0–\\xa0Да, батюшка,\\xa0– сказал Степан Аркадьич, покачивая головой,\\xa0– вот счастливец! Три тысячи десятин в Каразинском уезде, все впереди, и свежести сколько! Не то что наш брат.<br></br>\\xa0\\xa0\\xa0–\\xa0Что ж вы-то жалуетесь, Степан Аркадьич?<br></br>\\xa0\\xa0\\xa0–\\xa0Да скверно, плохо,\\xa0– сказал Степан Аркадьич, тяжело вздохнув.<br></br></div><h2 id=\"idm140335412628048\">VI\\n</h2><div style=\"text-align: justify\" class=\"hsection2\">\\xa0\\xa0\\xa0Когда Облонский спросил у Левина, зачем он, собственно, приехал, Левин покраснел и рассердился на себя за то, что покраснел, потому что он не мог ответить ему: «Я приехал сделать предложение твоей свояченице», хотя он приехал только за этим.<br></br>\\xa0\\xa0\\xa0Дом&#225; Левиных и Щербацких были старые дворянские мо'"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "html[100000:102000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-07-27T09:43:12.966015Z",
     "start_time": "2018-07-27T09:43:05.748105Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleaning\n"
     ]
    }
   ],
   "source": [
    "soup = BeautifulSoup(html)\n",
    "print(\"Cleaning\")\n",
    "ak_ru_txt = soup.find('body').get_text()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-07-27T09:43:12.981529Z",
     "start_time": "2018-07-27T09:43:12.971615Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nАнна Каренина\\nЛев Николаевич Толстой\\n\\n\\nРоман «Широкого дыхания»Часть перваяIIIIIIIVVVIVIIVIIIIXXXIXIIXIIIXIVXVXVIXVIIXVIIIXIXXXXXIXXIIXXIIIXXIVXXVXXVIXXVIIXXVIIIXXIXXXXXXXIXXXIIXXXIIIXXXIVЧасть втора'"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ak_ru_txt[:200]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-07-27T09:43:15.924121Z",
     "start_time": "2018-07-27T09:43:15.917859Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'>  A  =  C  2  H  0  O  A  O      ;  N  1  >  2  L  »   .       \"  >  ;  L  :  >      7  4  5  A  L      \"  >  ;  A  B  >  9      2  8  4  5  ;      3  0  @    >  =  8  N      8      A    K  A  ;      6  8  7  =  8      8      >  1  A  B  0  =  >  2  :  C      A  G  0  A  B  L  O  .       «     5  2  8  =      G  0  A  B  >      ;  N  1  >  2  0  ;  A  O      =  0      M  B  C      6  8  7  =  L  ,       G  0  A  B  >      8  A  ?  K  B  K  2  0  ;      G  C  2  A  B  2  >      7  0  2  8  A  B  8      :      ;  N  4  O    ,       6  8  2  C  I  8        M  B  >  9      6  8  7  =  L  N  ,       =  >      =  K  =  G  5      2      ?  5  @  2  K  9      @  0  7  ,       2      >  A  >  1  5  =  =  >  A  B  8      ?  >  4      2  ?  5  G  0  B  ;  5  =  8  5        B  >  3  >  ,       G  B  >      >  =      2  8  4  5  ;      2      >  B  =  >  H  5  =  8  O  E        2  0  =  0        0  @    5  =  >  2  0      :      A  2  >  5  9      6  5  =  5  ,         5  2  8  =  C      2      ?  5  @  2  K  9      @  0  7      O  A  =  >      ?  @  8  H  ;  0        K  A  ;  L      >      B  >    ,       G  B  >      >  B      =  5  3  >      7  0  2  8  A  8  B      ?  5  @  5    5  =  8  B  L      B  C      A  B  >  ;  L      B  O  3  >  A  B  =  C  N      8      ?  @  0  7  4  =  C  N  ,       8  A  :  C  A  A  B  2  5  =  =  C  N      8      ;  8  G  =  C  N      6  8  7  =  L  ,       :  >  B  >  @  >  N      >  =      6  8  ;  ,       =  0      M  B  C      B  @  C  4  >  2  C  N  ,       G  8  A  B  C  N      8      >  1  I  C  N      ?  @  5  ;  5  A  B  =  C  N      6  8  7  =  L  »   .         5  7        5  2  8  =  0      =  5      1  K  ;  >      1  K      8      @  >    0  =  0  .         A  B  >  @  8  O        =  =  K        0  @  5  =  8  =  >  9      >  1  @  5  B  0  5  B      A  2  >  5      =  0  A  B  >  O  I  5  5      7  =  0  G  5  =  8  5      2      A  2  5  B  5      4  C  E  >  2  =  K  E      8      A  >  F  8  0  ;  L  =  K  E      8  A  :  0  ='"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ak_ru_txt[100000:102000]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Нормализуем тексты"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-07-27T10:12:13.376697Z",
     "start_time": "2018-07-27T10:12:13.370919Z"
    }
   },
   "outputs": [],
   "source": [
    "import sklearn.feature_extraction.text\n",
    "import scipy.sparse as sp\n",
    "import nltk\n",
    "from tqdm import tqdm_notebook\n",
    "import pickle\n",
    "\n",
    "sentence_tokenizer = nltk.data.load('tokenizers/punkt/english.pickle')\n",
    "\n",
    "def prepare_sentences(text):\n",
    "\n",
    "    # список всех предложений в датасете\n",
    "    sentences_ru = []\n",
    "\n",
    "    review_sents = sentence_tokenizer.tokenize(replace_nums(remove_html(text)))\n",
    "    clean_sents = [lemmatize_text(tokenize(sentence)).split(\" \") for sentence in review_sents]\n",
    "\n",
    "    # список всех предложений в отзыве\n",
    "    sentences_ru.extend(clean_sents)\n",
    "    \n",
    "    return sentences_ru"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-07-27T10:12:23.967882Z",
     "start_time": "2018-07-27T10:12:14.634987Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['анна',\n",
       "  'каренин',\n",
       "  'левый',\n",
       "  'николай',\n",
       "  'толстой',\n",
       "  'роман',\n",
       "  '«',\n",
       "  'широкий',\n",
       "  'дыхание',\n",
       "  '»',\n",
       "  'часть',\n",
       "  'перваяiiiiiiivvviviiviiiixxxixiixiiixivxvxvixviixviiixixxxxxixxiixxiiixxivxxvxxvixxviixxviiixxixxxxxxxixxxiixxxiiixxxivчасть',\n",
       "  'втораяiiiiiiivvviviiviiiixxxixiixiiixivxvxvixviixviiixixxxxxixxiixxiiixxivxxvxxvixxviixxviiixxixxxxxxxixxxiixxxiiixxxivxxxvчасть',\n",
       "  'третьяiiiiiiivvviviiviiiixxxixiixiiixivxvxvixviixviiixixxxxxixxiixxiiixxivxxvxxvixxviixxviiixxixxxxxxxixxxiiчасть',\n",
       "  'четвертаяiiiiiiivvviviiviiiixxxixiixiiixivxvxvixviixviiixixxxxxixxiixxiiiчасть',\n",
       "  'пятаяiiiiiiivvviviiviiiixxxixiixiiixivxvxvixviixviiixixxxxxixxiixxiiixxivxxvxxvixxviixxviiixxixxxxxxxixxxiixxxiiiчасть',\n",
       "  'шестаяiiiiiiivvviviiviiiixxxixiixiiixivxvxvixviixviiixixxxxxixxiixxiiixxivxxvxxvixxviixxviiixxixxxxxxxixxxiiчасть',\n",
       "  'седьмаяiiiiiiivvviviiviiiixxxixiixiiixivxvxvixviixviiixixxxxxixxiixxiiixxivxxvxxvixxviixxviiixxixxxxxxxiчасть',\n",
       "  'восьмаяiiiiiiivvviviiviiiixxxixiixiiixivxvxvixviixviiixixый',\n",
       "  'романепримечание',\n",
       "  'левый',\n",
       "  'толстой',\n",
       "  'анна',\n",
       "  'каренин',\n",
       "  'роман',\n",
       "  '«',\n",
       "  'широкий',\n",
       "  'дыхание',\n",
       "  '»',\n",
       "  '«',\n",
       "  'анна',\n",
       "  'каренин',\n",
       "  '»',\n",
       "  'поразить',\n",
       "  'современник',\n",
       "  '«',\n",
       "  'вседневность',\n",
       "  'содержание',\n",
       "  '»',\n",
       "  '.']]"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ak_sentences = prepare_sentences(ak_ru_txt)\n",
    "ak_sentences[:1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ну такое"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Запускаем обучение"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-07-27T10:12:30.364974Z",
     "start_time": "2018-07-27T10:12:27.922431Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Word2Vec model...\n",
      "Model karenina__45features_2minwords_5context saved\n"
     ]
    }
   ],
   "source": [
    "w2v_ak = train_word2vec(\"karenina_\", ak_sentences, num_features=45, context=5, min_word_count=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-07-27T10:12:36.945415Z",
     "start_time": "2018-07-27T10:12:36.925362Z"
    }
   },
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "\"word 'любовь' not in vocabulary\"",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-123-5fa87dc05aef>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mw2v_ak\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msimilar_by_word\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"любовь\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/usr/local/lib/python3.6/site-packages/gensim/utils.py\u001b[0m in \u001b[0;36mnew_func1\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m   1417\u001b[0m                     \u001b[0mstacklevel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1418\u001b[0m                 )\n\u001b[0;32m-> 1419\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1420\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1421\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mnew_func1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/site-packages/gensim/models/base_any2vec.py\u001b[0m in \u001b[0;36msimilar_by_word\u001b[0;34m(self, word, topn, restrict_vocab)\u001b[0m\n\u001b[1;32m   1267\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1268\u001b[0m         \"\"\"\n\u001b[0;32m-> 1269\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msimilar_by_word\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mword\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtopn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrestrict_vocab\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1270\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1271\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mdeprecated\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Method will be removed in 4.0.0, use self.wv.similar_by_vector() instead\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/site-packages/gensim/models/keyedvectors.py\u001b[0m in \u001b[0;36msimilar_by_word\u001b[0;34m(self, word, topn, restrict_vocab)\u001b[0m\n\u001b[1;32m    569\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    570\u001b[0m         \"\"\"\n\u001b[0;32m--> 571\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmost_similar\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpositive\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mword\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtopn\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtopn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrestrict_vocab\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrestrict_vocab\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    572\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    573\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0msimilar_by_vector\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvector\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtopn\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrestrict_vocab\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/site-packages/gensim/models/keyedvectors.py\u001b[0m in \u001b[0;36mmost_similar\u001b[0;34m(self, positive, negative, topn, restrict_vocab, indexer)\u001b[0m\n\u001b[1;32m    528\u001b[0m                 \u001b[0mmean\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweight\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mword\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    529\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 530\u001b[0;31m                 \u001b[0mmean\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweight\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mword_vec\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mword\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0muse_norm\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    531\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mword\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvocab\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    532\u001b[0m                     \u001b[0mall_words\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvocab\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mword\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/site-packages/gensim/models/keyedvectors.py\u001b[0m in \u001b[0;36mword_vec\u001b[0;34m(self, word, use_norm)\u001b[0m\n\u001b[1;32m    449\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    450\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 451\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"word '%s' not in vocabulary\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mword\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    452\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    453\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget_vector\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mword\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: \"word 'любовь' not in vocabulary\""
     ]
    }
   ],
   "source": [
    "w2v_ak.similar_by_word(\"любовь\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Кластеризация word2vec-ов\n",
    "способ снизить размерность и объединить синонимы в одну фичу"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "words = list(w2v_ak.wv.vocab.keys())\n",
    "w2v_matrix = np.array([w2v_ak.wv[key] for key in words])\n",
    "w2v_matrix.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import MiniBatchKMeans\n",
    "\n",
    "CLUSTERS = 200\n",
    "\n",
    "clusterer = MiniBatchKMeans(n_clusters=CLUSTERS, verbose=0, init_size=500, random_state=124, batch_size=10000)\n",
    "labels = clusterer.fit_predict(w2v_matrix)\n",
    "labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# заполняем пустыми списками\n",
    "label2words = { label : [] for label in labels }\n",
    "words2label = {}\n",
    "i = 0\n",
    "\n",
    "for label in labels:\n",
    "    label2words[label].append(words[i])\n",
    "    words2label[words[i]] = label\n",
    "    i += 1\n",
    "    \n",
    "# распечатываем кластеры\n",
    "for label in label2words:\n",
    "    print(label)\n",
    "    print(\" \".join(label2words[label]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# IMDB: задачка для word2vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-07-27T10:28:08.656943Z",
     "start_time": "2018-07-27T10:28:06.993805Z"
    }
   },
   "outputs": [],
   "source": [
    "w2v_ggl = gensim.models.KeyedVectors.load(\"for_sharing/filtered-GoogleNews-vectors-negative300.bin\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-07-27T10:29:51.961728Z",
     "start_time": "2018-07-27T10:29:50.195104Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "import re \n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "train = pd.read_csv('for_sharing/imdb_data/labeledTrainData.tsv', header=0, delimiter=\"\\t\", quoting=3)\n",
    "test = pd.read_csv('for_sharing/imdb_data/testData.tsv', header=0, delimiter=\"\\t\", quoting=3)\n",
    "unlabeled_train = pd.read_csv('for_sharing/imdb_data/unlabeledTrainData.tsv', header=0,  delimiter=\"\\t\", quoting=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-07-27T10:29:52.528732Z",
     "start_time": "2018-07-27T10:29:52.521766Z"
    }
   },
   "outputs": [],
   "source": [
    "def review_to_words(raw_review, remove_stops=True):   \n",
    "    \n",
    "    review_text = BeautifulSoup(raw_review).get_text() \n",
    "    letters_only = re.sub(\"[^a-zA-Z]\", \" \", review_text) \n",
    "    words = letters_only.lower().split()     \n",
    "    \n",
    "    if remove_stops:\n",
    "        stops = set(stopwords.words(\"english\"))                 \n",
    "        return [w for w in words if not w in stops]  \n",
    "    else:\n",
    "        return words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-07-27T10:37:20.838471Z",
     "start_time": "2018-07-27T10:29:53.650433Z"
    },
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "61eca46e620b40699dfe63815e1bdd4b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=25000), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d12e07ca2f6245f4b0c8c121684516ec",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=50000), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-132-935d91856aa9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mreview\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtqdm_notebook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0munlabeled_train\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"review\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m     \u001b[0mall_sentences_available\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreview_to_sentences\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreview\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mremove_stopwords\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     23\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mall_sentences_available\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-132-935d91856aa9>\u001b[0m in \u001b[0;36mreview_to_sentences\u001b[0;34m(review, tokenizer, remove_stopwords)\u001b[0m\n\u001b[1;32m      6\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mraw_sentence\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mraw_sentences\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mraw_sentence\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m                 \u001b[0msentences\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreview_to_words\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mraw_sentence\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mremove_stopwords\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0msentences\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-131-fa43e1330403>\u001b[0m in \u001b[0;36mreview_to_words\u001b[0;34m(raw_review, remove_stops)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mreview_to_words\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mraw_review\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mremove_stops\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m     \u001b[0mreview_text\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mBeautifulSoup\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mraw_review\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m     \u001b[0mletters_only\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mre\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msub\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"[^a-zA-Z]\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\" \"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreview_text\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mwords\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mletters_only\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/site-packages/bs4/__init__.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, markup, features, builder, parse_only, from_encoding, exclude_encodings, **kwargs)\u001b[0m\n\u001b[1;32m    226\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    227\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 228\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_feed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    229\u001b[0m                 \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    230\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mParserRejectedMarkup\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/site-packages/bs4/__init__.py\u001b[0m in \u001b[0;36m_feed\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    287\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuilder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    288\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 289\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuilder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfeed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmarkup\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    290\u001b[0m         \u001b[0;31m# Close out any unfinished strings and close all the open tags.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    291\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mendData\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/site-packages/bs4/builder/_lxml.py\u001b[0m in \u001b[0;36mfeed\u001b[0;34m(self, markup)\u001b[0m\n\u001b[1;32m    249\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparser_for\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mencoding\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    250\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparser\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfeed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmarkup\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 251\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparser\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    252\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mUnicodeDecodeError\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mLookupError\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0metree\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mParserError\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    253\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mParserRejectedMarkup\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32msrc/lxml/parser.pxi\u001b[0m in \u001b[0;36mlxml.etree._FeedParser.close\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32msrc/lxml/parser.pxi\u001b[0m in \u001b[0;36mlxml.etree._FeedParser.close\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32msrc/lxml/parsertarget.pxi\u001b[0m in \u001b[0;36mlxml.etree._TargetParserContext._handleParseResult\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32msrc/lxml/parsertarget.pxi\u001b[0m in \u001b[0;36mlxml.etree._TargetParserContext._handleParseResult\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32msrc/lxml/etree.pyx\u001b[0m in \u001b[0;36mlxml.etree._ExceptionContext._raise_if_stored\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32msrc/lxml/saxparser.pxi\u001b[0m in \u001b[0;36mlxml.etree._handleSaxEndNoNs\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32msrc/lxml/parsertarget.pxi\u001b[0m in \u001b[0;36mlxml.etree._PythonSaxParserTarget._handleSaxEnd\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/site-packages/bs4/builder/_lxml.py\u001b[0m in \u001b[0;36mend\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m    193\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    194\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 195\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msoup\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mendData\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    196\u001b[0m         \u001b[0mcompleted_tag\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msoup\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtagStack\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    197\u001b[0m         \u001b[0mnamespace\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_getNsTag\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/site-packages/bs4/__init__.py\u001b[0m in \u001b[0;36mendData\u001b[0;34m(self, containerClass)\u001b[0m\n\u001b[1;32m    362\u001b[0m                 \u001b[0;32mreturn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    363\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 364\u001b[0;31m             \u001b[0mo\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcontainerClass\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcurrent_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    365\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mobject_was_parsed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mo\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    366\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/site-packages/bs4/element.py\u001b[0m in \u001b[0;36m__new__\u001b[0;34m(cls, value)\u001b[0m\n\u001b[1;32m    702\u001b[0m     \u001b[0mknown_xml\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    703\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 704\u001b[0;31m     \u001b[0;32mdef\u001b[0m \u001b[0m__new__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcls\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    705\u001b[0m         \"\"\"Create a new NavigableString.\n\u001b[1;32m    706\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "def review_to_sentences(review, tokenizer, remove_stopwords=False):\n",
    "    \n",
    "        raw_sentences = tokenizer.tokenize(review.strip())\n",
    "        sentences = []\n",
    "\n",
    "        for raw_sentence in raw_sentences:        \n",
    "            if len(raw_sentence) > 0:\n",
    "                sentences.append(review_to_words(raw_sentence, remove_stopwords))\n",
    "                \n",
    "        return sentences\n",
    "\n",
    "\n",
    "from tqdm import tqdm, tqdm_notebook\n",
    "\n",
    "tokenizer = nltk.data.load('tokenizers/punkt/english.pickle')\n",
    "all_sentences_available = []\n",
    "\n",
    "for review in tqdm_notebook(train[\"review\"]):\n",
    "    all_sentences_available.extend(review_to_sentences(review, tokenizer, remove_stopwords=True))\n",
    "\n",
    "for review in tqdm_notebook(unlabeled_train[\"review\"]):\n",
    "    all_sentences_available.extend(review_to_sentences(review, tokenizer, remove_stopwords=True))\n",
    "\n",
    "len(all_sentences_available)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pickle\n",
    "# with open(\"all_sentences.bin\", \"wb\") as wf:\n",
    "#     pickle.dump(all_sentences_available, wf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_sentences_available = pickle.load(open(\"for_sharing/all_sentences.bin\", \"rb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Word2Vec model...\n",
      "Model imdb_300features_5minwords_10context saved\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<gensim.models.word2vec.Word2Vec at 0x7ffa31637438>"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w2v_imdb = train_word2vec(\"imdb\", all_sentences_available)\n",
    "w2v_imdb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open(\"w2v_imdb\", \"wb\") as wf:\n",
    "#     pickle.dump(w2v_imdb, wf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "w2v_imdb = pickle.load(open(\"w2v_imdb\", \"rb\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Начинаем готовить датасет для классификации по средним векторам"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "def avg_vector_by_review(words, model):\n",
    "    \n",
    "    accumulator = np.zeros((model.vector_size,), dtype=\"float32\")\n",
    "    found_count = 0.\n",
    "    not_found = 0.\n",
    "    \n",
    "    for word in words:\n",
    "        if word in model.wv:\n",
    "            found_count += 1\n",
    "            accumulator +=  model.wv[word]\n",
    "        else:\n",
    "            not_found += 1\n",
    "    \n",
    "#     print(\"Not found percentage:\", not_found / (not_found + found_count))\n",
    "    \n",
    "    return accumulator / found_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "def avg_vectors_for_dataset(reviews, model):\n",
    "    return np.matrix([avg_vector_by_review(review_to_words(review), model) for review in tqdm_notebook(reviews)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Считаем средние векторы для всех отзывов"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      " 17%|█▋        | 4198/25000 [00:27<02:16, 152.02it/s]\u001b[A\u001b[A\u001b[A"
     ]
    },
    {
     "data": {
      "text/plain": [
       "((25000, 300), (25000, 300))"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_vecs = avg_vectors_for_dataset(train[\"review\"], w2v_ggl)\n",
    "test_vecs = avg_vectors_for_dataset(test[\"review\"], w2v_ggl)\n",
    "\n",
    "train_vecs.shape, test_vecs.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Будем предсказывать на СРЕДНИХ векторных представлениях слов"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn\n",
    "from sklearn.model_selection import GridSearchCV, cross_val_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "def main_train(model, param_grid, train_vecs, y, test_vecs):\n",
    "\n",
    "    # перебор гиперпараметров по сетке; по дефолту кросс-валидация StratifiedKFold\n",
    "    clf = GridSearchCV(estimator=model, param_grid=param_grid, n_jobs=7, verbose=1)\n",
    "\n",
    "    # отложим в сторону holdout, чтобы убедиться, \n",
    "    # что с нашими оценками на kfold всё в порядке\n",
    "    X_train, X_ho, y_train, y_ho = train_test_split(train_vecs, \n",
    "                                                    train.sentiment, \n",
    "                                                    test_size=0.2, \n",
    "                                                    random_state=42)\n",
    "    clf.fit(X_train, y_train)\n",
    "\n",
    "    print(\"Best score:\", clf.best_score_)\n",
    "    print(\"Best params:\", clf.best_params_)\n",
    "\n",
    "    # задаём модели лучшие найденные параметры\n",
    "    model = clf.best_estimator_\n",
    "\n",
    "    # обучаем на всём, кроме холдаута\n",
    "    model = model.fit(X_train, y_train)\n",
    "\n",
    "    # смотрим на качество предсказаний на холдауте\n",
    "    # должно быть похоже на оценку от поиска по сетке\n",
    "    print(\"Holdout score:\", model.score(X_ho, y_ho), \"-- is it close to the validation score?\")\n",
    "\n",
    "    # обучаем модель на всей размеченной выборке\n",
    "    model.fit(train_vecs, train.sentiment)\n",
    "    result = model.predict(test_vecs)\n",
    "\n",
    "    \n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-07-27T10:37:20.843563Z",
     "start_time": "2018-07-27T10:35:21.941Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import SGDClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 24 candidates, totalling 72 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=7)]: Done  36 tasks      | elapsed:    3.2s\n",
      "[Parallel(n_jobs=7)]: Done  72 out of  72 | elapsed:    6.2s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best score: 0.8514\n",
      "Best params: {'average': True, 'penalty': 'l2', 'loss': 'modified_huber'}\n",
      "Holdout score: 0.8554 -- is it close to the validation score?\n"
     ]
    }
   ],
   "source": [
    "model = SGDClassifier(n_jobs=-1)\n",
    "\n",
    "# надо ещё перебрать соотв. числа!\n",
    "penalties = [ \"l1\", \"l2\", \"elasticnet\"]\n",
    "losses = [ \"modified_huber\", \"log\", \"huber\", \"hinge\" ]\n",
    "\n",
    "# какие параметры рассмотрим\n",
    "param_grid = { \"penalty\": penalties, \"average\": [True, False], \"loss\": losses  }\n",
    "\n",
    "result = main_train(model, param_grid, train_vecs, train.sentiment, test_vecs)\n",
    "\n",
    "# запись сабмишшена для кегла\n",
    "output = pd.DataFrame(data={\"id\":test[\"id\"], \"sentiment\":result})\n",
    "output.to_csv(\"sgd-submission_wordvec_avg.csv\", index=False, quoting=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### А теперь давайте использовать word2vec-и по-умному\n",
    "\n",
    "На основе многоканальной архитектуры Yoon Kim\n",
    "\n",
    "https://github.com/castorini/Castor/tree/master/kim_cnn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "class KimCNN(nn.Module):\n",
    "    \n",
    "    def __init__(self, vectors, dropout_rate, input_channel, output_channel, target_class, maxlen):\n",
    "        \n",
    "        super(KimCNN, self).__init__()\n",
    "        \n",
    "        # число свёрток с разными окнами\n",
    "        Ks = 3 \n",
    "        \n",
    "        self.non_static_embed = nn.Embedding.from_pretrained(vectors, freeze=False)\n",
    "\n",
    "        self.conv1 = nn.Conv1d(input_channel, output_channel, ???, padding=(???, 0))\n",
    "        self.conv2 = nn.Conv1d(input_channel, output_channel, ???, padding=(???, 0))\n",
    "        self.conv3 = nn.Conv1d(input_channel, output_channel, ???, padding=(???, 0))\n",
    "\n",
    "        self.dropout = nn.Dropout(dropout_rate)\n",
    "        self.fc1 = nn.Linear(Ks * output_channel, target_class)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \n",
    "        non_static_input = self.non_static_embed(x)\n",
    "        \n",
    "        print()\n",
    "        print(non_static_input.shape)\n",
    "        \n",
    "        # (batch, channel_input, sent_len, embed_dim)\n",
    "        x = non_static_input.unsqueeze(???) \n",
    "        \n",
    "        print(x.shape)\n",
    "        \n",
    "        # применяем свёртку к данным, и вычисляем функцию активации ReLU\n",
    "        x = [F.relu(self.conv1(x)), #.squeeze(???), \n",
    "             F.relu(self.conv2(x)), #.squeeze(???), \n",
    "             F.relu(self.conv3(x))] #.squeeze(???)]\n",
    "        \n",
    "        # (batch, channel_output, ~=sent_len) * Ks\n",
    "        x = [F.max_pool1d(i, i.size(???)).squeeze(???) for i in x] # max-over-time pooling\n",
    "        \n",
    "        # (batch, channel_output) * Ks\n",
    "        x = torch.cat(x, 1) \n",
    "        \n",
    "        print(x.shape)\n",
    "        x = self.dropout(x)\n",
    "        \n",
    "        print(x.shape)\n",
    "        \n",
    "        # (batch, target_size)\n",
    "        logit = self.fc1(x) \n",
    "        \n",
    "        return logit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [],
   "source": [
    "def word_ids_by_review(words, model, word2id, max_len):\n",
    "    \n",
    "    accumulator = []\n",
    "    found_count = 0.\n",
    "    not_found = 0.\n",
    "    \n",
    "    for word in words[:max_len]:\n",
    "        if word in model.wv:\n",
    "            accumulator.append(word2id[word])\n",
    "        else:\n",
    "            accumulator.append(-1)\n",
    "    \n",
    "    for _ in range(0, max_len - len(words)):\n",
    "        accumulator.append(-1)\n",
    "    \n",
    "    accumulator = np.array(accumulator)\n",
    "    \n",
    "    return accumulator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Строим матрицу векторов"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(37065, 300)"
      ]
     },
     "execution_count": 187,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embeddings = []\n",
    "\n",
    "word2ix = {}\n",
    "\n",
    "for id, w in enumerate(filtered_w2v_ggl.wv.vocab):\n",
    "    embeddings.append(filtered_w2v_ggl.wv[w])\n",
    "    word2ix[w] = id\n",
    "\n",
    "embeddings = np.array(embeddings)\n",
    "\n",
    "embeddings.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import optim\n",
    "\n",
    "losses = []\n",
    "loss_function = nn.NLLLoss()\n",
    "vocab_size = len(word2index)\n",
    "\n",
    "EMBEDDING_DIM = 300\n",
    "HIDDEN_DIM = 300\n",
    "MAX_LEN = 50\n",
    "\n",
    "cnn_model = KimCNN(dropout_rate=0.4, \n",
    "                   input_channel=1, \n",
    "                   output_channel=1, \n",
    "                   target_class=1,\n",
    "                   vectors=torch.tensor(embeddings),\n",
    "                   maxlen=MAX_LEN).cuda()\n",
    "\n",
    "optimizer = optim.Adam(cnn_model.parameters(), lr=0.001)\n",
    "\n",
    "cnn_model, optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm_notebook\n",
    "import numpy as np\n",
    "\n",
    "EPOCHS = 2\n",
    "\n",
    "def train_routine(model, loss_function, batches, epochs=30):\n",
    "\n",
    "    for epoch in range(EPOCHS):\n",
    "\n",
    "        total_loss = 0\n",
    "        count = 0\n",
    "\n",
    "        for features_batch, target_batch in tqdm_notebook(batches):\n",
    "\n",
    "            word_vectors_reviews = torch.tensor(features_batch, dtype=torch.long).cuda()\n",
    "\n",
    "            # градиенты надо сбрасывать, если не хотим аккумулировать\n",
    "            model.zero_grad()\n",
    "\n",
    "            # применяем модель\n",
    "            log_probs = model(word_vectors_reviews)\n",
    "\n",
    "            # вычисляем невязку\n",
    "            loss = loss_function(log_probs, torch.tensor(target_batch, dtype=torch.long).cuda())\n",
    "\n",
    "            # обратный проход, обновление градиента\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            # получаем число\n",
    "            total_loss += loss.item()\n",
    "            count += 1\n",
    "\n",
    "        print(\"E\", epoch + 1, \"\\tNLL\\t\", total_loss / count)\n",
    "\n",
    "        losses.append(total_loss)\n",
    "        \n",
    "    return model, losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [],
   "source": [
    "def chunks(l0, l1, n):\n",
    "    \n",
    "    assert len(l0) == len(l1)\n",
    "    coll0, coll1 = [], []\n",
    "    \n",
    "    for i in tqdm_notebook(range(0, len(l0), n)):\n",
    "        coll0.append(l0[i:i + n])\n",
    "        coll1.append(l1[i:i + n])\n",
    "        \n",
    "    return coll0, coll1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([32326,    -1,    -1,    -1,    -1,    -1,    -1,    -1,    -1,\n",
       "          -1,    -1,    -1,    -1,    -1,    -1,    -1,    -1,    -1,\n",
       "          -1,    -1,    -1,    -1,    -1,    -1,    -1,    -1,    -1,\n",
       "          -1,    -1,    -1,    -1,    -1,    -1,    -1,    -1,    -1,\n",
       "          -1,    -1,    -1,    -1,    -1,    -1,    -1,    -1,    -1,\n",
       "          -1,    -1,    -1,    -1,    -1])"
      ]
     },
     "execution_count": 191,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_ids_by_review(review_to_words(\"hello\"), filtered_w2v_ggl, word2ix, max_len=MAX_LEN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = [word_ids_by_review(review_to_words(txt), filtered_w2v_ggl, word2ix, MAX_LEN) for txt in train[\"review\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(25000, 25000)"
      ]
     },
     "execution_count": 193,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(data), len(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2500, 2500)"
      ]
     },
     "execution_count": 194,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batched_texts, batched_targets = chunks(data, list(train.sentiment), n=10)\n",
    "\n",
    "len(batched_texts), len(batched_targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [],
   "source": [
    "# batched_texts[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "cuda runtime error (59) : device-side assert triggered at /pytorch/aten/src/THC/generic/THCTensorCopy.c:20",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-197-9ac35f8fd013>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mbatches\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatched_texts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatched_targets\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mtrain_routine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcnn_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss_function\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatches\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-189-fd7aef0d7ebf>\u001b[0m in \u001b[0;36mtrain_routine\u001b[0;34m(model, loss_function, batches, epochs)\u001b[0m\n\u001b[1;32m     13\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mfeatures_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_batch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtqdm_notebook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m             \u001b[0mword_vectors_reviews\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeatures_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlong\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m             \u001b[0;31m# градиенты надо сбрасывать, если не хотим аккумулировать\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: cuda runtime error (59) : device-side assert triggered at /pytorch/aten/src/THC/generic/THCTensorCopy.c:20"
     ]
    }
   ],
   "source": [
    "batches = list(zip(batched_texts, batched_targets))\n",
    "\n",
    "train_routine(cnn_model, loss_function, batches, epochs=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  },
  "toc": {
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": "block",
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
